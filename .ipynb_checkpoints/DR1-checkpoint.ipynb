{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dragonx/venv3/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "import pickle\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "from data import load_augment\n",
    "random.seed(334)\n",
    "\n",
    "class data_loader(data.Dataset):\n",
    "    def __init__(self, inputPath, ifTrain, transform = False):\n",
    "        self.input_path = inputPath\n",
    "        self.transform = transform\n",
    "        self.file_list_total = os.listdir(inputPath)\n",
    "        if ifTrain:\n",
    "            self.file_list = self.file_list_total[: int(0.7*len(self.file_list_total))]\n",
    "        else:\n",
    "            self.file_list = self.file_list_total[int(0.7*len(self.file_list_total)): ]\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.input_path + '/' + self.file_list[idx]\n",
    "        w= 224\n",
    "        h= 224\n",
    "        aug_params= {\n",
    "            'zoom_range': (1 / 1.15, 1.15),\n",
    "            'rotation_range': (0, 360),\n",
    "            'shear_range': (0, 0),\n",
    "            'translation_range': (-20, 20),\n",
    "            'do_flip': True,\n",
    "            'allow_stretch': True,\n",
    "        }\n",
    "        sigma= 0.25\n",
    "        image = load_augment(fname, w, h, aug_params=aug_params, transform=None, sigma=sigma, color_vec=None)\n",
    "        #print('after', image.shape)\n",
    "        data = h5py.File(self.input_path + '/' + self.file_list[idx], 'r')\n",
    "        #image = data['image'].value\n",
    "        #print('before', image.shape)\n",
    "        #target = float(data['target'].value)\n",
    "        one_hot = np.zeros(5)\n",
    "        one_hot[target] = 1\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        #return image, torch.from_numpy(np.array([target]))\n",
    "        return image, torch.from_numpy(one_hot)\n",
    "        #return image, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.nn' has no attribute 'Conv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-602eb1b2220a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0mdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0mdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_512_4_4_32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-602eb1b2220a>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#border_mode=None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mdrago2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#border_mode=None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.nn' has no attribute 'Conv'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\"\"\"\n",
    "For nonlinearity, we use leaky (0.01) rectifier units following each convolutional layer. \n",
    "The networks are trained with Nesterov momentum with fixed schedule over 250 epochs. For the nets \n",
    "on 256 and 128 pixel images, we stop training after 200 epochs. L2 weight decay with factor 0.0005\n",
    "are applied to all layers. \n",
    "The problem is treated as a regression problem, the loss function is mean squared error.\n",
    "\"\"\"\n",
    "class c_512_4_4_32(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(c_512_4_4_32, self).__init__()\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=(3,3),stride=2)\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=(3,3), stride=2)\n",
    "        self.dropout = nn.Dropout(p= 0.5)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.01)\n",
    "#         # net 4\n",
    "#         self.conv1_1 = nn.Conv2d(3, 32, kernel_size=(4,4), stride=2)\n",
    "#         self.conv1_2 = nn.Conv2d(32, 32, kernel_size=(4,4), padding=2) #border_mode=None\n",
    "\n",
    "#         self.conv2_1 = nn.Conv2d(32, 64, kernel_size=(4,4), stride=2)\n",
    "#         self.conv2_2 = nn.Conv2d(64, 64, kernel_size=(4, 4), padding=2) #border_mode=None\n",
    "#         self.conv2_3 = nn.Conv2d(64, 64, kernel_size=(4, 4))\n",
    "\n",
    "#         self.conv3_1 = nn.Conv2d(64, 128, kernel_size=(4, 4), padding=2) #border_mode=None\n",
    "#         self.conv3_2 = nn.Conv2d(128, 128, kernel_size=(4, 4))\n",
    "#         self.conv3_3 = nn.Conv2d(128, 128, kernel_size=(4, 4), padding=2) #border_mode=None\n",
    "\n",
    "#         self.conv4_1 = nn.Conv2d(128, 256, kernel_size=(4, 4), padding=2) #border_mode=None\n",
    "#         self.conv4_2 = nn.Conv2d(256, 256, kernel_size=(4, 4))\n",
    "#         self.conv4_3 = nn.Conv2d(256, 256, kernel_size=(4, 4), padding=2) #border_mode=None\n",
    "\n",
    "#         self.conv5_1 = nn.Conv2d(256, 512, kernel_size=(4, 4), padding=3)\n",
    "\n",
    "        # net 5\n",
    "        self.conv1_1 = nn.Conv2d(3, 32, kernel_size=(5,5), stride=2)\n",
    "        self.conv1_2 = nn.Conv2d(32, 32, kernel_size=(3,3), padding=2) #border_mode=None\n",
    "\n",
    "        self.conv2_1 = nn.Conv2d(32, 64, kernel_size=(5,5), stride=2)\n",
    "        self.conv2_2 = nn.Conv2d(64, 64, kernel_size=(3, 3), padding=2) #border_mode=None\n",
    "        self.conv2_3 = nn.Conv2d(64, 64, kernel_size=(3, 3))\n",
    "\n",
    "        self.conv3_1 = nn.Conv2d(64, 128,  kernel_size=(3, 3), padding=2) #border_mode=None\n",
    "        self.conv3_2 = nn.Conv2d(128, 128, kernel_size=(3, 3))\n",
    "        self.conv3_3 = nn.Conv2d(128, 128, kernel_size=(3, 3), padding=2) #border_mode=None\n",
    "\n",
    "        self.conv4_1 = nn.Conv2d(128, 256, kernel_size=(3, 3), padding=2) #border_mode=None\n",
    "        self.conv4_2 = nn.Conv2d(256, 256, kernel_size=(3, 3))\n",
    "        self.conv4_3 = nn.Conv2d(256, 256, kernel_size=(3, 3), padding=2) #border_mode=None\n",
    "\n",
    "        self.conv5_1 = nn.Conv2d(256, 512, kernel_size=(3, 3))\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=(3, 3))\n",
    "        \n",
    "        self.fc_1 = nn.Linear(256, 1)\n",
    "        #x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "\n",
    "        #self.ap = nn.AvgPool2d(3, stride=(2, 2))\n",
    "        '''\n",
    "        self.fc_1 = nn.Linear(1024)\n",
    "\n",
    "        self.fc_2 = nn.Linear(1024)\n",
    "\n",
    "        self.mo = nn.MaxOut(1024, 1024, 2)\n",
    "\n",
    "        self.RMSPool = torch.sqrt(nn.AvgPool2d(torch.sqrt(x), kernel_size=3, stride=(2,2)) + 1e-12)\n",
    "        '''\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.max_pool(self.leaky_relu(self.conv1_2(self.leaky_relu(self.conv1_1(x)))))\n",
    "        print(x.size())\n",
    "        x = self.leaky_relu(self.conv2_3(self.leaky_relu(self.conv2_2(self.leaky_relu(self.conv2_1(x))))))\n",
    "        print(x.size())\n",
    "        #x = self.max_pool(self.conv3_3(self.conv3_2(self.conv3_1(x))))\n",
    "        x = self.leaky_relu(self.conv3_3(self.leaky_relu(self.conv3_2(self.leaky_relu(self.conv3_1(x))))))\n",
    "        print(x.size())\n",
    "        x = self.leaky_relu(self.conv4_3(self.leaky_relu(self.conv4_2(self.leaky_relu(self.conv4_1(x))))))\n",
    "        print(x.size())\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        #x = (self.conv5_1(x))\n",
    "        print(x.size())\n",
    "        #x = torch.sqrt(self.ap(torch.sqrt(x)) + 1e-12)  #RMSPool\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_1(x)\n",
    "        print(x.size())\n",
    "        '''\n",
    "        x = self.mo(self.fc_1(self.dropout(x)))\n",
    "        print(x.size())\n",
    "        x = self.mo(self.fc_2(self.dropout(x)))\n",
    "        print(x.size())\n",
    "        '''\n",
    "        return x\n",
    "\n",
    "\n",
    "# class Maxout(nn.Module):\n",
    "\n",
    "#     def __init__(self, d_in, d_out, pool_size):\n",
    "#         super().__init__()\n",
    "#         self.d_in, self.d_out, self.pool_size = d_in, d_out, pool_size\n",
    "#         self.lin = nn.Linear(d_in, d_out * pool_size)\n",
    "#     def forward(self, inputs):\n",
    "#         shape = list(inputs.size())\n",
    "#         shape[-1] = self.d_out\n",
    "#         shape.append(self.pool_size)\n",
    "#         max_dim = len(shape) - 1\n",
    "#         out = self.lin(inputs)\n",
    "#         m, i = out.view(*shape).max(max_dim)\n",
    "#         return m\n",
    "\n",
    "dd = torch.randn(1, 3, 512, 512)\n",
    "dd = torch.autograd.Variable(dd)\n",
    "model = c_512_4_4_32()\n",
    "y = model(dd)\n",
    "print (y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": [
     23,
     28
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(arch='resnet18', batch_size=256, epochs=10000000, evaluate=False, finetune=False, lr=0.1, momentum=0.9, ng_weights=0.1, no_cuda=False, pretrained=False, print_freq=10, resume='', start_epoch=0, weight_decay=0.0005, workers=4)\n"
     ]
    }
   ],
   "source": [
    "############Data Import and Training############\n",
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "#from classify_loader import data_loader\n",
    "\n",
    "# available model names \n",
    "model_names = sorted(name for name in models.__dict__\n",
    "    if name.islower() and not name.startswith(\"__\"))\n",
    "print(model_names)\n",
    "\n",
    "# define args to control the whole training procedure\n",
    "parser = argparse.ArgumentParser(description='PyTorch ImageNet Training')\n",
    "#parser.add_argument('data', metavar='DIR',\n",
    "#                    help='path to dataset')\n",
    "parser.add_argument('--arch', '-a', metavar='ARCH', default='resnet18',\n",
    "                    #choices=model_names,\n",
    "                    help='model architecture: ' +\n",
    "                        ' | '.join(model_names) +\n",
    "                        ' (default: resnet18)')\n",
    "parser.add_argument('-j', '--workers', default=4, type=int, metavar='N',\n",
    "                    help='number of data loading workers (default: 4)')\n",
    "parser.add_argument('--epochs', default=10000000, type=int, metavar='N',\n",
    "                    help='number of total epochs to run')\n",
    "parser.add_argument('--start-epoch', default=0, type=int, metavar='N',\n",
    "                    help='manual epoch number (useful on restarts)')\n",
    "parser.add_argument('-b', '--batch-size', default=256, type=int,\n",
    "                    metavar='N', help='mini-batch size (default: 256)')\n",
    "parser.add_argument('--lr', '--learning-rate', default=0.1, type=float,\n",
    "                    metavar='LR', help='initial learning rate')\n",
    "parser.add_argument('--momentum', default=0.9, type=float, metavar='M',\n",
    "                    help='momentum')\n",
    "parser.add_argument('--weight-decay', '--wd', default=5e-4, type=float,\n",
    "                    metavar='W', help='weight decay (default: 1e-4)')\n",
    "parser.add_argument('--print-freq', '-p', default=10, type=int,\n",
    "                    metavar='N', help='print frequency (default: 10)')\n",
    "parser.add_argument('--resume', default='', type=str, metavar='PATH',\n",
    "                    help='path to latest checkpoint (default: none)')\n",
    "parser.add_argument('-e', '--evaluate', dest='evaluate', action='store_true',\n",
    "                    help='evaluate model on validation set')\n",
    "parser.add_argument('--pretrained', dest='pretrained', action='store_true',\n",
    "                    help='use pre-trained model')\n",
    "parser.add_argument('--finetune', dest='finetune', action='store_true',\n",
    "                    help='fine tune pre-trained model')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='disables CUDA training')\n",
    "parser.add_argument('--ng-weights', type=float, default=0.1)\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "print(args)\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a RAM model\n",
    "Model = c_512_4_4_32()\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        data_loader('/data/jeffery/kg/sample_512/', True, transform=\n",
    "                    transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        #transforms.Normalize((-257.478639,), (471.683592,)),\n",
    "                    ])\n",
    "        ),\n",
    "        batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    data_loader('/data/jeffery/kg/sample_512/', False, transform=\n",
    "    transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize((-257.478639,), (471.683592,)),\n",
    "    ])\n",
    "                ),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    #prec = AverageMeter()\n",
    "    #top1 = AverageMeter()\n",
    "    #top5 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        #target = target.cuda(async=True)\n",
    "        input_var = torch.autograd.Variable(input).cuda(async=True)\n",
    "        #print(target.numpy())\n",
    "        #target_var = torch.autograd.Variable(target.type(torch.LongTensor)).cuda(async=True)\n",
    "        target_var = torch.autograd.Variable(target.type(torch.FloatTensor)).cuda(async=True)\n",
    "        #print(type(target_var.data))\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        #print ('output', output.size())\n",
    "        #print ('target', target_var.size())\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        #prec1, prec5 = accuracy(output.data, target)\n",
    "        #prec_cur = accuracy(output.data, target)\n",
    "        losses.update(loss.data[0], input.size(0))\n",
    "        #top1.update(prec1[0], input.size(0))\n",
    "        #top5.update(prec5[0], input.size(0))\n",
    "        #prec.update(prec_cur[0], input.size(0))\n",
    "\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Train Epoch: [{}][{}/{}]\\t'\n",
    "                  #'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  #'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\n'\n",
    "                  #'Prec {prec.val:.3f}'\n",
    "                  #'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  #'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'\n",
    "                .format(\n",
    "                   epoch, i*len(input), len(train_loader.dataset),\n",
    "                   #batch_time=batch_time,\n",
    "                   #data_time=data_time,\n",
    "                   loss=losses\n",
    "                  # prec = prec\n",
    "                #top1=top1, top5=top5\n",
    "\n",
    "            ))\n",
    "\n",
    "def accu_for_cls(predict, target, cls):\n",
    "    target_np = target.data.numpy().cpu()\n",
    "    predict_np = predict.data.numpy().cpu()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in range(len(target_np)):\n",
    "        if target_np[i] == np.array(cls):\n",
    "            if target_np[i] == predict_np[i]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    if total != 0:\n",
    "        return correct/ total, total\n",
    "    else:\n",
    "        return 0, 0\n",
    "\n",
    "def validate(val_loader, model, criterion):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    tprs = AverageMeter()\n",
    "    accs = AverageMeter()\n",
    "    accs_0 = AverageMeter()\n",
    "    accs_1 = AverageMeter()\n",
    "    accs_2 = AverageMeter()\n",
    "    accs_3 = AverageMeter()\n",
    "    accs_4 = AverageMeter()\n",
    "    #top1 = AverageMeter()\n",
    "    #top5 = AverageMeter()\n",
    "    #prec = AverageMeter\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(val_loader):\n",
    "        #target = target.cuda(async=True)\n",
    "        input_var = torch.autograd.Variable(input, volatile=True).cuda(async=True)\n",
    "        #target_var = torch.autograd.Variable(target.type(torch.LongTensor), volatile=True).cuda(async=True)\n",
    "        target_var = torch.autograd.Variable(target.type(torch.FloatTensor), volatile=True).cuda(async=True)\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "        tpr = true_positive(output.data, target_var.data)\n",
    "        acc = accuracy(output.data, target_var.data)\n",
    "        # measure accuracy and record loss\n",
    "        #prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        losses.update(loss.data[0], input.size(0))\n",
    "        tprs.update(tpr, input.size(0))\n",
    "        accs.update(acc, input.size(0))\n",
    "        #top1.update(prec1[0], input.size(0))\n",
    "        #top5.update(prec5[0], input.size(0))\n",
    "        #prec.update(prec[0], input.size(0))\n",
    "        acc_0, num_0 = accu_for_cls(output.data, target_var.data, [1,0,0,0,0])\n",
    "        acc_1, num_1 = accu_for_cls(output.data, target_var.data, [0,1,0,0,0])\n",
    "        acc_2, num_2 = accu_for_cls(output.data, target_var.data, [0,0,1,0,0])\n",
    "        acc_3, num_3 = accu_for_cls(output.data, target_var.data, [0,0,0,1,0])\n",
    "        acc_4, num_4 = accu_for_cls(output.data, target_var.data, [0,0,0,0,1])\n",
    "        accs_0.update(acc_0, num_0)\n",
    "        accs_1.update(acc_1, num_1)\n",
    "        accs_2.update(acc_2, num_2)\n",
    "        accs_3.update(acc_3, num_3)\n",
    "        accs_4.update(acc_4, num_4)\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "    print('Test: [Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'accuracy {acc.val:.4f} ({acc.avg:.4f})\\t'\n",
    "                  'tpr {tpr.val:.4f} ({tpr.avg:.4f})'\n",
    "                  #'Prec {prec.val:.3f}'\n",
    "                  #'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  #'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'\n",
    "            .format(\n",
    "                   batch_time=batch_time, loss=losses,\n",
    "                   acc = accs, tpr = tprs\n",
    "                   #top1=top1, top5=top5\n",
    "                   #prec = prec\n",
    "                 ))\n",
    "\n",
    "    #print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}'\n",
    "    #      .format(top1=top1, top5=top5))\n",
    "\n",
    "    #return top1.avg\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function (criterion) and pptimizer\n",
    "\n",
    "weight = torch.ones(num_classes)\n",
    "weight[0] = args.ng_weights\n",
    "#criterion = nn.CrossEntropyLoss(weight.cuda()).cuda()\n",
    "#criterion = torch.nn.MultiMarginLoss(weight = weight.cuda()).cuda()\n",
    "criterion = torch.nn.MultiMarginLoss().cuda()\n",
    "#criterion = torch.nn.CrossEntropyLoss(weight.cuda()).cuda()\n",
    "#criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "#criterion = torch.nn.MSELoss().cuda()\n",
    "optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), # Only finetunable params\n",
    "                            args.lr,\n",
    "                            momentum=args.momentum,\n",
    "                            weight_decay=args.weight_decay)\n",
    "\n",
    "if args.evaluate:\n",
    "    validate(test_loader, model, criterion)\n",
    "    return\n",
    "\n",
    "for epoch in range(args.start_epoch, args.epochs):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "    # train for one epoch\n",
    "    train(train_loader, model, criterion, optimizer, epoch)\n",
    "\n",
    "    # evaluate on validation set\n",
    "    loss = validate(test_loader, model, criterion)\n",
    "\n",
    "    # remember best prec@1 and save checkpoint\n",
    "    #is_best = prec1 > best_prec1\n",
    "    #best_prec1 = max(prec1, best_prec1)\n",
    "    is_best = loss < best_loss\n",
    "    best_loss = min(loss, best_loss)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'arch': args.arch,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_loss': best_loss,\n",
    "    }, is_best)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
